{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1:\n",
        "\n",
        "**Correct:** C \\\\\n",
        "$x_1$(time of the day) is ordinal \\\n",
        " $x_6$ (Traffic lights) is ratio, \\\n",
        " $x_7$ (Running over) is ratio, \\\n",
        "  $y$ (Congestion level) is ordinal"
      ],
      "metadata": {
        "id": "Aa3wRs-bclhr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2:\n",
        "\n",
        "**Correct:** A\n",
        "it's corrrect by the definition of $p-norm$ while $p=\n",
        "\\infty$ being the maximum of the absolute differences between corresponding elements."
      ],
      "metadata": {
        "id": "3siHlUVIdXDa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3:\n",
        "**Correct answer A** \\\\\n",
        "The matrix $V$ contains the principal components, and the diagonal matrix $S$\n",
        "contains singular values, which correspond to the variance explained by each principal component. The variance explained by each component is proportional to the square of its singular value. The total variance is the sum of the squares of all singular values. These are the first steps we need to do.\n",
        "We compute the total variance, i.e :\n",
        "$S_{total} =  13.9^2 + 12.47^2 + 11.48^2 + 10.03^2 + 9.45^2 = 670.4$\n",
        "\n",
        "Next, we'll compute the proportion of variance explained by different numbers of principal components.\n",
        "Doing the math on paper,\n",
        "\n",
        "$Variance_{ first 4 components} = 13.9^2 + 12.47^2 + 11.48^2 + 10.03^2 = 581.3 $ \\\\\n",
        " variance explained = $\\frac{581.13}{670.4} \\approx 0.867$ ,We used $(3.18)$ from the book.\n",
        "\n"
      ],
      "metadata": {
        "id": "LgTQqBkDeMfu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4:\n",
        "**Correct D** \\\\\n",
        "*solution*:  \\\\\n",
        "$V =\n",
        "\\begin{bmatrix}\n",
        "0.49 & -0.5  & 0.08 & -0.49 & 0.52 \\\\\n",
        "0.58 & 0.23  & -0.01 & 0.71 & 0.33 \\\\\n",
        "0.56 & 0.23  & 0.43 & -0.25 & -0.62 \\\\\n",
        "0.31 & 0.09  & -0.9 & -0.19 & -0.24 \\\\\n",
        "-0.06 & 0.8  & 0.03 & -0.41 & 0.43 \\end{bmatrix}$\n",
        "\n",
        "In PCA, the matrix $V$ contains the eigenvectors (principal components) and tells us how much each original feature contributes to each principal component. Each row of this matrix corresponds to a feature (e.g., Time of day, Broken Truck, etc.), and each column corresponds to a principal component. So to make it more clear, The first row corresponds to **Time of day**, the second row to **Broken Truck** etc.. \\\\\n",
        "Each element in the matrix tells us how much the corresponding feature contributes to a given principal component. For example, the first element in the first column (0.49) tells us that Time of day has a weight of 0.49 for PC1. \\\n",
        "\n",
        "\n",
        "We need to check how each feature contributes to PC2. The second column of\n",
        "$V$ gives us the weights for PC2: \\\\\n",
        "**Time of day has** a weight of -0.5. A low value of Time of day (since the weight is negative) will result in an increase in the projection onto PC2.\n",
        "\\\n",
        "**Broken Truck** has a weight of 0.23. A high value of Broken Truck (positive weight) will increase the projection onto PC2. \\\n",
        "**Accident victim** has a weight of 0.23. A high value of Accident victim (positive weight) will increase the projection onto PC2. \\\n",
        "**Defects** has a weight of 0.8. A high value of Defects (positive weight) will increase the projection onto PC2. \\\n",
        "\n",
        "Since all these contributions either increase the projection or reduce it in a very minor way(this is because we have no information on the **Immobilized bus** feature), the overall projection onto PC2 will likely be positive, making Statement D true.\n",
        "\n",
        "p.s  although Immobilized bus has a positive weight, its small magnitude (0.09) means it doesn't significantly increase the projection compared to features with higher weights (like Defects, 0.8). This is why we say it \"increases the projection in a minor way.\""
      ],
      "metadata": {
        "id": "b43xfVT6x7k-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5:\n",
        "**Correct A** \\\\\n",
        "Question 5 asks about the Jaccard similarity between two text documents $s_1$ and $s_2$.\\\n",
        "â€‹The Jaccard similarity measures the similarity between two sets by comparing the size of their intersection to the size of their union.\n",
        "\n",
        "The Jaccard similarity is defined as: $J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}$  in our case $A = s_1$ and $B = s_2$ \\\\\n",
        "$ A \\cup B = \\{ \\text{the, bag, of, words, representation, becomes, less, parsimonious, if, we, do, not, stem} \\} $ \\\\\n",
        "$|A \\cup B| = 13 $ \\\\\n",
        "\n",
        "$A \\cap B = \\{ \\text{the, words} \\} $ \\\\\n",
        "$ |A \\cap B|=2 $ \\\\\n",
        "$ J(s_1, s_2) = \\frac{|A \\cap B|}{|A \\cup B|} = \\frac{2}{13} \\approx 0.153846$\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2u8HRYqe9rfc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 6:\n",
        "\n",
        "We are interested in $p(\\hat{x}_2 = 0 | y = 2)$ , which can be computed by summing over the possible values of $\\hat{x}_7$:  \\\n",
        "\n",
        "$p(\\hat{x}_2 = 0 | y = 2) = p(\\hat{x}_2 = 0, \\hat{x}_7 = 0 | y = 2) + p(\\hat{x}_2 = 0, \\hat{x}_7 = 1 | y = 2)$ \\\n",
        "We have, $p(\\hat{x}_2 = 0, \\hat{x}_7 = 0 | y = 2) = 0.81$ and \\\n",
        "$p(\\hat{x}_2 = 0, \\hat{x}_7 = 1 | y = 2) = 0.03$\n",
        "\n",
        "so, $p(\\hat{x}_2 = 0 | y=2) = 0.84$\n",
        "\n",
        "\n",
        "In practice we made use of the law of Total Probability,see: [Law of Total Probability](https://en.wikipedia.org/wiki/Law_of_total_probability):\n",
        "\n"
      ],
      "metadata": {
        "id": "cwkvrksrvRP0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KbRGbg_cYp-"
      },
      "outputs": [],
      "source": []
    }
  ]
}
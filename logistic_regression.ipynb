{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, make_scorer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fetch data\n",
    "df = pd.read_csv(\"preprocessed_log_data.csv\")\n",
    "df.head()"
   ],
   "id": "d6fea1e42d60f965",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df['energy_mark'] = df['energy_mark'].apply(lambda x: x[0]).map({'A':'Energy class A','B':'Lower classes','C':'Lower classes','D':'Lower classes','E':'Lower classes','F':'Lower classes','G':'Lower classes','n':'None'})\n",
    "\n",
    "X = df.drop(columns=['energy_mark']).copy()\n",
    "y = df['energy_mark'].copy()\n",
    "y"
   ],
   "id": "497cf83150f2c88a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Remove columns\n",
    "print(X.columns.to_list())"
   ],
   "id": "e1e531cb077055e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Transformation of predictors\n",
    "\n",
    "# Standardization\n",
    "cols_to_standardize = df.select_dtypes(include=['float64']).columns.to_list()\n",
    "cols_to_standardize = [col for col in cols_to_standardize]\n",
    "scaler = StandardScaler()\n",
    "X[cols_to_standardize] = scaler.fit_transform(X[cols_to_standardize])\n",
    "\n",
    "# Make the categorical variables into dummies\n",
    "X = pd.get_dummies(X)"
   ],
   "id": "f327042e4a968cd3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get only rows that have some energy mark (985 rows)\n",
    "\n",
    "# rows_with_energy_mark = y.notnull()\n",
    "# print(rows_with_energy_mark)\n",
    "# indices = [i for i in range(len(rows_with_energy_mark)) if rows_with_energy_mark[i] == True]\n",
    "# print(indices)\n",
    "\n",
    "rows_with_energy_mark = y[y != 'None']\n",
    "indices = list(rows_with_energy_mark.index)\n",
    "X_with_energy_mark = X.loc[indices]\n",
    "y_with_energy_mark = y.loc[indices]\n",
    "y_with_energy_mark"
   ],
   "id": "eddae6163620dd3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Split to train and validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_with_energy_mark, y_with_energy_mark, test_size=0.10, shuffle=True, stratify=y_with_energy_mark)"
   ],
   "id": "272c14ee9833793a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Logistic regression with l2 regularization\n",
    "\n",
    "lmbda = 1\n",
    "\n",
    "lr_model = LogisticRegression(penalty='l2', C=(1/lmbda), max_iter=1000).fit(X_train, y_train)\n",
    "y_pred = lr_model.predict(X_test)\n",
    "lr_accuracy = accuracy_score(y_test,y_pred)\n",
    "print(\"Logistic regression accuracy: {}\".format(lr_accuracy))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))"
   ],
   "id": "8636273ab42ad92e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Baseline model\n",
    "class BaselineModel:\n",
    "    def __init__(self, prediction_value = None):\n",
    "        self.prediction_value = prediction_value\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.prediction_value = y.value_counts().idxmax()\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return pd.Series(self.prediction_value, index=X.index)\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        # Return parameters as a dictionary\n",
    "        return {\"prediction_value\": self.prediction_value}\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        # Set parameters from a dictionary\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "        return self\n",
    "    \n",
    "baseline_model = BaselineModel().fit(X_train, y_train)\n",
    "y_pred = baseline_model.predict(X_test)\n",
    "baseline_accuracy = accuracy_score(y_test,y_pred)\n",
    "print(\"Baseline accuracy: {}\".format(baseline_accuracy))"
   ],
   "id": "e937d0d6dc76b291",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Define the Random Forest model\n",
    "# You can tune n_estimators and other parameters based on your dataset\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_rf_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "rf_accuracy = accuracy_score(y_test, y_rf_pred)\n",
    "print(\"Random Forest accuracy: {}\".format(rf_accuracy))\n",
    "\n",
    "# Display confusion matrix\n",
    "print(\"Random Forest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_rf_pred))"
   ],
   "id": "8a7134fe7ec35075",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Two level cross validation (setup)\n",
    "n = 10\n",
    "outer_fold = KFold(n_splits=n, shuffle=True)\n",
    "inner_fold = KFold(n_splits=n, shuffle=True)\n",
    "\n",
    "classifiers = [\n",
    "    LogisticRegression(penalty='l2', max_iter=1000),\n",
    "    BaselineModel(),\n",
    "    RandomForestClassifier(random_state=42)\n",
    "]\n",
    "\n",
    "params = {\n",
    "    classifiers[0].__class__.__name__: {\"C\": [10000,1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001]},\n",
    "    classifiers[1].__class__.__name__: {},\n",
    "    classifiers[2].__class__.__name__: {\"n_estimators\": [10,50,100,200,300,400,500,750,1000,1500]}\n",
    "}\n",
    "\n",
    "def calculate_error(y_true: pd.Series, y_pred: pd.Series):\n",
    "    # Calculate the number of misclassified samples\n",
    "    n_misclassified = np.sum(y_true != y_pred)\n",
    "    # Calculate the proportion of misclassified samples\n",
    "    error_rate = n_misclassified / len(y_true)\n",
    "    return error_rate\n",
    "\n",
    "error_scorer = make_scorer(calculate_error, greater_is_better=False)\n",
    "\n",
    "predictions = {key:[] for key in params.keys()}\n",
    "test_errors = {key:[] for key in params.keys()}\n",
    "targets = []"
   ],
   "id": "811726480680b7a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for train_idx, test_idx in tqdm(outer_fold.split(X_with_energy_mark)):\n",
    "    X_train, X_test = X_with_energy_mark.iloc[train_idx], X_with_energy_mark.iloc[test_idx]\n",
    "    y_train, y_test = y_with_energy_mark.iloc[train_idx], y_with_energy_mark.iloc[test_idx]\n",
    "    \n",
    "    targets.append(y_test)\n",
    "\n",
    "    for classifier in classifiers:\n",
    "        # Nested CV with parameter optimization\n",
    "        clf = GridSearchCV(\n",
    "            estimator=classifier, \n",
    "            param_grid=params[classifier.__class__.__name__], \n",
    "            cv=inner_fold, \n",
    "            scoring=error_scorer\n",
    "        )\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        best_estimator = clf.best_estimator_\n",
    "        y_pred = best_estimator.predict(X_test)\n",
    "        error = calculate_error(y_test, y_pred)\n",
    "        \n",
    "        predictions[classifier.__class__.__name__].append(y_pred)\n",
    "        test_errors[classifier.__class__.__name__].append((clf.best_params_, error))\n",
    "\n",
    "print(test_errors)"
   ],
   "id": "52133deee04b37b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualise test_errors in dataframe\n",
    "\n",
    "errors_df = pd.DataFrame(columns=[\"Outer fold\", \"Random_Forest_n_estimators\", \"Random_Forest_test_error\", \"LR_lambda\", \"LR_test_error\", \"Baseline_test_error\"])\n",
    "errors_df[\"Outer fold\"] = range(10)\n",
    "errors_df[[\"Random_Forest_n_estimators\", 'Random_Forest_test_error']] = \\\n",
    "    [(item[0]['n_estimators'], item[1]) for item in test_errors['RandomForestClassifier']]\n",
    "errors_df[[\"LR_lambda\", 'LR_test_error']] = \\\n",
    "    [(1 / item[0]['C'], item[1]) for item in test_errors['LogisticRegression']]\n",
    "errors_df['Baseline_test_error'] = \\\n",
    "    [item[1] for item in test_errors['BaselineModel']]\n",
    "\n",
    "errors_df.astype({'Random_Forest_n_estimators': 'int32'})"
   ],
   "id": "1787c27593b3516c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Statistical evaluation of two methods\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "def mcnemar_statistical_evaluation(e1, e2, alpha=0.05):\n",
    "    # Convert inputs to numpy arrays for easier manipulation\n",
    "    e1 = np.array(e1)\n",
    "    e2 = np.array(e2)\n",
    "    \n",
    "    # Calculate differences in errors for each fold\n",
    "    diff = e1 - e2\n",
    "    \n",
    "    # Calculate mean and standard error of the difference\n",
    "    mean_diff = np.mean(diff)\n",
    "    std_diff = np.std(diff, ddof=1)\n",
    "    n = len(diff)\n",
    "    std_err_diff = std_diff / np.sqrt(n)\n",
    "    \n",
    "    # 95% confidence interval\n",
    "    t_critical = stats.t.ppf(1 - alpha/2, df=n-1)\n",
    "    margin_of_error = t_critical * std_err_diff\n",
    "    ci_lower = mean_diff - margin_of_error\n",
    "    ci_upper = mean_diff + margin_of_error\n",
    "    \n",
    "    # Paired t-test for p-value\n",
    "    t_stat, p_value = stats.ttest_rel(e1, e2)\n",
    "    \n",
    "    # Output results\n",
    "    print(f\"95% Confidence Interval for Mean Difference: [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "    print(f\"P-value for paired t-test: {p_value:.7f}\")"
   ],
   "id": "60dc1c25548c7510",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"------ Logistic Regression and Baseline Comparison ------\\n\")\n",
    "mcnemar_statistical_evaluation(errors_df['LR_test_error'], errors_df['Baseline_test_error'])\n",
    "print(\"\\n\\n\\n------ Random Forest and Baseline Comparison ------\\n\")\n",
    "mcnemar_statistical_evaluation(errors_df['Random_Forest_test_error'], errors_df['Baseline_test_error'])\n",
    "print(\"\\n\\n\\n------ Logistic Regression and Random Forest Comparison ------\\n\")\n",
    "mcnemar_statistical_evaluation(errors_df['LR_test_error'], errors_df['Random_Forest_test_error'])"
   ],
   "id": "f1d519f5783ec51e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Split to train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_with_energy_mark, y_with_energy_mark, test_size=0.10, shuffle=True, stratify=y_with_energy_mark)\n",
    "\n",
    "# Train logistic regression classifier with the best parameter found from cross validation\n",
    "lmbda = 0.1\n",
    "\n",
    "lr_model = LogisticRegression(penalty='l2', C=(1/lmbda), max_iter=1000).fit(X_train, y_train)\n",
    "y_pred = lr_model.predict(X_test)\n",
    "lr_accuracy = accuracy_score(y_test,y_pred)\n",
    "print(\"Logistic regression accuracy: {}\".format(lr_accuracy))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))"
   ],
   "id": "3e157d4a8cdbdb1a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "29ac37c1c246ec2c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro_to_ml",
   "language": "python",
   "name": "intro_to_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
